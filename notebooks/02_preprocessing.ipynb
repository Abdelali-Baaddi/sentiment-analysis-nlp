{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4869adb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Preprocessing for NLP Pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dda51b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING DATA\n",
      "==================================================\n",
      "Original dataset shape: (50000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"LOADING DATA\")\n",
    "print(\"=\"*50)\n",
    "df = pd.read_csv('../data/raw/IMDB_Dataset.csv')\n",
    "print(f\"Original dataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5310781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "REMOVING DUPLICATES\n",
      "==================================================\n",
      "Duplicates found: 418\n",
      "Shape after removing duplicates: (49582, 2)\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"REMOVING DUPLICATES\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Duplicates found: {df.duplicated().sum()}\")\n",
    "df = df.drop_duplicates()\n",
    "print(f\"Shape after removing duplicates: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03d3638b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "HANDLING MISSING VALUES\n",
      "==================================================\n",
      "Missing values:\n",
      "review       0\n",
      "sentiment    0\n",
      "dtype: int64\n",
      "Shape after removing missing values: (49582, 2)\n"
     ]
    }
   ],
   "source": [
    "# Handle missing values\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"HANDLING MISSING VALUES\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Missing values:\\n{df.isnull().sum()}\")\n",
    "df = df.dropna()\n",
    "print(f\"Shape after removing missing values: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "213c3f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Preprocessing Functions\n",
    "def remove_html_tags(text):\n",
    "    \"\"\"Remove HTML tags from text\"\"\"\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', text)\n",
    "\n",
    "def remove_urls(text):\n",
    "    \"\"\"Remove URLs from text\"\"\"\n",
    "    return re.sub(r'http\\S+|www.\\S+', '', text)\n",
    "\n",
    "def remove_special_chars(text):\n",
    "    \"\"\"Remove special characters and digits\"\"\"\n",
    "    return re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "def convert_to_lowercase(text):\n",
    "    \"\"\"Convert text to lowercase\"\"\"\n",
    "    return text.lower()\n",
    "\n",
    "def remove_extra_whitespace(text):\n",
    "    \"\"\"Remove extra whitespace\"\"\"\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"Remove stopwords\"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = text.split()\n",
    "    return ' '.join([word for word in words if word not in stop_words])\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    \"\"\"Lemmatize text\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = text.split()\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in words])\n",
    "\n",
    "def stem_text(text):\n",
    "    \"\"\"Stem text\"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    words = text.split()\n",
    "    return ' '.join([stemmer.stem(word) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88283ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "PREPROCESSING TEXT\n",
      "==================================================\n",
      "Step 1: Removing HTML tags...\n",
      "Step 2: Removing URLs...\n",
      "Step 3: Converting to lowercase...\n",
      "Step 4: Removing special characters and digits...\n",
      "Step 5: Removing extra whitespace...\n",
      "Step 6: Removing stopwords...\n",
      "Step 7: Lemmatizing text...\n",
      "\n",
      "✓ Preprocessing completed!\n"
     ]
    }
   ],
   "source": [
    "# Apply preprocessing pipeline\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PREPROCESSING TEXT\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create a copy for preprocessing\n",
    "df['cleaned_review'] = df['review'].copy()\n",
    "\n",
    "print(\"Step 1: Removing HTML tags...\")\n",
    "df['cleaned_review'] = df['cleaned_review'].apply(remove_html_tags)\n",
    "\n",
    "print(\"Step 2: Removing URLs...\")\n",
    "df['cleaned_review'] = df['cleaned_review'].apply(remove_urls)\n",
    "\n",
    "print(\"Step 3: Converting to lowercase...\")\n",
    "df['cleaned_review'] = df['cleaned_review'].apply(convert_to_lowercase)\n",
    "\n",
    "print(\"Step 4: Removing special characters and digits...\")\n",
    "df['cleaned_review'] = df['cleaned_review'].apply(remove_special_chars)\n",
    "\n",
    "print(\"Step 5: Removing extra whitespace...\")\n",
    "df['cleaned_review'] = df['cleaned_review'].apply(remove_extra_whitespace)\n",
    "\n",
    "print(\"Step 6: Removing stopwords...\")\n",
    "df['cleaned_review'] = df['cleaned_review'].apply(remove_stopwords)\n",
    "\n",
    "print(\"Step 7: Lemmatizing text...\")\n",
    "df['cleaned_review'] = df['cleaned_review'].apply(lemmatize_text)\n",
    "\n",
    "print(\"\\n✓ Preprocessing completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5321f1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "SAMPLE COMPARISON\n",
      "==================================================\n",
      "\n",
      "Original Review:\n",
      "One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me abo...\n",
      "\n",
      "Cleaned Review:\n",
      "one reviewer mentioned watching oz episode youll hooked right exactly happened methe first thing struck oz brutality unflinching scene violence set right word go trust show faint hearted timid show pu...\n"
     ]
    }
   ],
   "source": [
    "# Display sample\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SAMPLE COMPARISON\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\nOriginal Review:\")\n",
    "print(df['review'].iloc[0][:200] + \"...\")\n",
    "print(\"\\nCleaned Review:\")\n",
    "print(df['cleaned_review'].iloc[0][:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08c2c464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ENCODING LABELS\n",
      "==================================================\n",
      "Label distribution:\n",
      "label\n",
      "1    24884\n",
      "0    24698\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Encode labels\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ENCODING LABELS\")\n",
    "print(\"=\"*50)\n",
    "df['label'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n",
    "print(f\"Label distribution:\\n{df['label'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bed0d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove empty reviews after preprocessing\n",
    "df = df[df['cleaned_review'].str.strip().str.len() > 0]\n",
    "print(f\"\\nShape after removing empty reviews: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0c22e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "SPLITTING DATA\n",
      "==================================================\n",
      "Training set size: 34707\n",
      "Validation set size: 7437\n",
      "Test set size: 7438\n",
      "\n",
      "Training set distribution:\n",
      "label\n",
      "1    17419\n",
      "0    17288\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Validation set distribution:\n",
      "label\n",
      "1    3732\n",
      "0    3705\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test set distribution:\n",
      "label\n",
      "1    3733\n",
      "0    3705\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SPLITTING DATA\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "X = df['cleaned_review']\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Validation set size: {len(X_val)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")\n",
    "\n",
    "print(f\"\\nTraining set distribution:\\n{y_train.value_counts()}\")\n",
    "print(f\"\\nValidation set distribution:\\n{y_val.value_counts()}\")\n",
    "print(f\"\\nTest set distribution:\\n{y_test.value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfd8ce3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "SAVING PROCESSED DATA\n",
      "==================================================\n",
      "✓ Full processed data saved\n"
     ]
    }
   ],
   "source": [
    "# Save processed data\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SAVING PROCESSED DATA\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Save full processed dataset\n",
    "df[['review', 'cleaned_review', 'sentiment', 'label']].to_csv(\n",
    "    '../data/processed/processed_data.csv', index=False\n",
    ")\n",
    "print(\"✓ Full processed data saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1c72b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Train/Val/Test splits saved\n"
     ]
    }
   ],
   "source": [
    "# Save train/val/test splits\n",
    "train_df = pd.DataFrame({'text': X_train, 'label': y_train})\n",
    "val_df = pd.DataFrame({'text': X_val, 'label': y_val})\n",
    "test_df = pd.DataFrame({'text': X_test, 'label': y_test})\n",
    "\n",
    "train_df.to_csv('../data/processed/train.csv', index=False)\n",
    "val_df.to_csv('../data/processed/val.csv', index=False)\n",
    "test_df.to_csv('../data/processed/test.csv', index=False)\n",
    "\n",
    "print(\"✓ Train/Val/Test splits saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e78471d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Preprocessing configuration saved\n",
      "\n",
      "==================================================\n",
      "PREPROCESSING COMPLETE\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Save preprocessing configuration\n",
    "preprocessing_config = {\n",
    "    'steps': [\n",
    "        'remove_html_tags',\n",
    "        'remove_urls',\n",
    "        'convert_to_lowercase',\n",
    "        'remove_special_chars',\n",
    "        'remove_extra_whitespace',\n",
    "        'remove_stopwords',\n",
    "        'lemmatize_text'\n",
    "    ],\n",
    "    'label_encoding': {'positive': 1, 'negative': 0}\n",
    "}\n",
    "\n",
    "with open('../models/preprocessing_config.pkl', 'wb') as f:\n",
    "    pickle.dump(preprocessing_config, f)\n",
    "\n",
    "print(\"✓ Preprocessing configuration saved\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PREPROCESSING COMPLETE\")\n",
    "print(\"=\"*50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
